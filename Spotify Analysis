package OmriPlayGround

import org.apache.log4j._
import org.apache.spark.sql.{DataFrame, Dataset, SparkSession, functions}
import org.apache.spark.sql.functions._
import org.apache.spark.ml.regression.DecisionTreeRegressor
import org.apache.spark.ml.feature.VectorAssembler

/** Compute the total amount spent per customer in some fake e-commerce data. */
object SpotifyReviews {

  case class SpotifyAnalysis(Time_submitted: String, Review: String, Rating: String, Total_thumbsup: String, Reply: String)

  // Get movie name by given dataset and id
  def AddMonthAndYearCols(schemaSpotify: Dataset[SpotifyAnalysis]): DataFrame = {

    val AddMonthAndYear = schemaSpotify.select( col("Time_submitted"))

    val NewDF = AddMonthAndYear.toDF("input")
      .select( col("input"),
        month(col("input")).as("month"),
        year(col("input")).as("year")
      )

    val NewSchema = NewDF.withColumn("Rating",functions.lit(1))
    NewSchema
  }

  /** Our main function where the action happens */
  def main(args: Array[String]) {
   
    // Set the log level to only print errors
    Logger.getLogger("org").setLevel(Level.ERROR)

    val spark = SparkSession
      .builder
      .appName("SpotifyReviewsAnalysis")
      .master("local[*]")
      .getOrCreate()

    // Load each line of the source data into an Dataset
    import spark.implicits._
    val schemaSpotify = spark.read
      .option("header", "true")
      .option("inferSchema", "true")
      .csv("data/SportifyReviews.csv")
      .as[SpotifyAnalysis]

    val totalByDate = schemaSpotify
      .groupBy("Time_submitted")
      .agg(sum("Total_thumbsup")
        .alias("total_likes"))

    val totalByDateSorted = totalByDate.sort(desc("total_likes"))

    println("Total likes by date: ")
    totalByDateSorted.show(totalByDate.count.toInt)

    // Add Month and Year columns
    val MonthAndYearSchema = AddMonthAndYearCols(schemaSpotify: Dataset[SpotifyAnalysis])

    val AvgByMonth = MonthAndYearSchema
      .groupBy("Month")
      .agg(avg("Rating")
        .alias("Rating_AVG"))

    println("Rating average by Month: ")
    AvgByMonth.sort(asc("Month")).show()

    val AvgByYear = MonthAndYearSchema
      .groupBy("Year")
      .agg(avg("Rating")
        .alias("Rating_AVG"))

    val AvgByYearSorted = AvgByYear.sort(desc("Year"))

    println("Rating average by Year: ")
    AvgByYearSorted.show(AvgByYear.count.toInt)

    // ReviewsWordCount
    // Split using a regular expression that extracts words
    val words = schemaSpotify
      .select(explode(split($"Review", "\\W+")).alias("word"))
      .filter($"word" =!= "")

    // Normalize everything to lowercase
    val lowercaseWords = words.select(lower($"word").alias("word"))

    // Count up the occurrences of each word
    val wordCounts = lowercaseWords.groupBy("word").count()

    // Sort by counts
    val wordCountsSorted = wordCounts.sort(desc("count"))

    // Show the results.
    println("Words appearances:")
    wordCountsSorted.show(wordCountsSorted.count.toInt)

    val dates = schemaSpotify.select( col("Time_submitted"))

    val NewDF = dates.toDF("input")
      .select( col("input"),
        month(col("input")).as("month"),
        year(col("input")).as("year"),
        dayofmonth(col("input")).as("dayofmonth"),
        dayofweek(col("input")).as("dayofweek")
      )

    val NewDS = NewDF.withColumn("rating",functions.lit(1))
    val NewDS2 = NewDS.withColumn("total_likes",functions.lit(1))

    //Decision Tree Algorithm

    import spark.implicits._
    val assembler = new VectorAssembler().
      setInputCols(Array("dayofweek", "dayofmonth", "year", "month", "Total_thumbsup")).
      setOutputCol("features")
    val df = assembler.transform(NewDS2)
      .select("Rating","features")

    val results: List[Double] = List(0.4, 0.5, 0.6)
    for (x <- results) {
      println("Train group size is:" + x)
      // Let's split our data into training data and testing data
      val trainTest = df.randomSplit(Array(x, 1 - x))
      val trainingDF = trainTest(0)
      val testDF = trainTest(1)

      // Now create our DecisionTree model
      val DTR = new DecisionTreeRegressor()
        .setFeaturesCol("features")
        .setLabelCol("Rating")

      // Train the model using our training data
      val model = DTR.fit(trainingDF)

      // Now see if we can predict values in our test data.
      // Generate predictions using our Decision Tree model for all features in our
      // test dataframe:
      val fullPredictions = model.transform(testDF).cache()

      // This basically adds a "prediction" column to our testDF dataframe.

      // Extract the predictions and the "known" correct labels.
      val predictionAndLabel = fullPredictions.select("prediction", "Rating").collect()


      // Print out the predicted and actual values for each point
      for (prediction <- predictionAndLabel) {
        println(prediction)
      }
    }
    // Stop the session
    spark.stop()
  }
  
}

