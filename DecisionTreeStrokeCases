package OmriPlayGround

import org.apache.log4j._
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.regression.DecisionTreeRegressor
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._


object DecisionTree {
  
  case class RegressionSchema(gender: String,	age: Double,	hypertension: Integer,
                              heart_disease: Integer,	ever_married: String,	work_type: String,
                              Residence_type: String,	avg_glucose_level: Double,	bmi: Double,
                              smoking_status: String,	stroke: Integer)


  /** Our main function where the action happens */
  def main(args: Array[String]) {
    // Set the log level to only print errors
    Logger.getLogger("org").setLevel(Level.ERROR)

    val spark = SparkSession
      .builder
      .appName("DecisionTree")
      .master("local[*]")
      .getOrCreate()
      
    // Load up our page speed / amount spent data in the format required by MLLib
    // (which is label, vector of features)
      
    // In machine learning lingo, "label" is just the value you're trying to predict, and
    // "feature" is the data you are given to make a prediction with. So in this example
    // the "labels" are the first column of our data, and "features" are the second column.

    val regressionSchema = new StructType()
      .add("label", DoubleType, nullable = true)
      .add("features_raw", DoubleType, nullable = true)

    import spark.implicits._
    val dsRaw = spark.read
      .option("sep", ",")
      .option("header", "true")
      .option("inferSchema", "true")
      .csv("data/stroke/full_data_stroke.csv")
      .as[RegressionSchema]

    val assembler = new VectorAssembler().
      setInputCols(Array("hypertension", "age", "heart_disease")).
      setOutputCol("features")
    val df = assembler.transform(dsRaw)
        .select("stroke","features")

    val results: List[Double] = List(0.3, 0.4, 0.5, 0.6, 0.7)
    for (x <- results) {
      println("Train group size is:" + x)
      // Let's split our data into training data and testing data
      val trainTest = df.randomSplit(Array(x, 1 - x))
      val trainingDF = trainTest(0)
      val testDF = trainTest(1)

      // Now create our DecisionTree model
      val DTR = new DecisionTreeRegressor()
        .setFeaturesCol("features")
        .setLabelCol("stroke")

      // Train the model using our training data
      val model = DTR.fit(trainingDF)

      // Now see if we can predict values in our test data.
      // Generate predictions using our linear regression model for all features in our
      // test dataframe:
      val fullPredictions = model.transform(testDF).cache()

      // This basically adds a "prediction" column to our testDF dataframe.

      // Extract the predictions and the "known" correct labels.
      val predictionAndLabel = fullPredictions.select("prediction", "stroke").collect()


      // Print out the predicted and actual values for each point
      for (prediction <- predictionAndLabel) {
        println(prediction)
      }
    }
    // Stop the session
    spark.stop()
  }
}


